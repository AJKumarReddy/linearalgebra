{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Linear Algebra for ML — FAANG-Level Lab\n",
        "\n",
        "**Goal:** Build shape intuition + compute core linear algebra objects used in ML.\n",
        "\n",
        "**Outcome:** You can implement least squares, projections, eigen decomposition intuition, and SVD-based PCA.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def check(name: str, cond: bool):\n",
        "    if not cond:\n",
        "        raise AssertionError(f'Failed: {name}')\n",
        "    print(f'OK: {name}')\n",
        "\n",
        "rng = np.random.default_rng(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1 — Vectors, Dot Product, Norms\n",
        "\n",
        "### Task 1.1: Implement dot + L2 norm (no np.linalg.norm)\n",
        "\n",
        "# HINT:\n",
        "- dot(x,y) = sum(x_i * y_i)\n",
        "- ||x||_2 = sqrt(dot(x,x))\n",
        "\n",
        "**Explain:** What does dot product measure geometrically?\n",
        "\n",
        "it takes direction into account and measures the projection of one vector on to another vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OK: dot\n",
            "OK: norm\n"
          ]
        }
      ],
      "source": [
        "def dot(x, y):\n",
        "    # TODO\n",
        "    return sum(x * y)\n",
        "\n",
        "def l2_norm(x):\n",
        "    # TODO\n",
        "    return np.sqrt(sum(x * x))\n",
        "\n",
        "x = np.array([1., 2., 3.])\n",
        "y = np.array([4., 5., 6.])\n",
        "check('dot', abs(dot(x,y) - 32.0) < 1e-9)\n",
        "check('norm', abs(l2_norm(x) - np.sqrt(14.0)) < 1e-9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2 — Matrix Multiplication + Shapes\n",
        "\n",
        "### Task 2.1: Validate shapes and compute A@B\n",
        "\n",
        "Given A (n,d) and B (d,k) compute C (n,k).\n",
        "\n",
        "**FAANG gotcha:** Many bugs are shape bugs. Always assert shapes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OK: C_shape\n",
            "OK: matmul_close\n"
          ]
        }
      ],
      "source": [
        "A = rng.standard_normal((5, 3))\n",
        "B = rng.standard_normal((3, 2))\n",
        "\n",
        "# TODO: compute C\n",
        "C = A @ B # Other ways to do it are C = np.dot(A, B) or C = np.matmul(A, B)\n",
        "\n",
        "check('C_shape', C.shape == (5, 2))\n",
        "check('matmul_close', np.allclose(C, A @ B))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3 — Projections (Least Squares Intuition)\n",
        "\n",
        "### Task 3.1: Project vector v onto vector u\n",
        "\n",
        "proj_u(v) = (u^T v / u^T u) * u\n",
        "\n",
        "# HINT:\n",
        "- Use your dot()\n",
        "\n",
        "**Explain:** Why does projection show up in linear regression?\n",
        "\n",
        "it is because projection is used to get the shortest distance which helps in finding the error. by recuding the overall error across all data points we can arraive at the best fit model for a given data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OK: proj\n"
          ]
        }
      ],
      "source": [
        "def proj(u, v):\n",
        "    # TODO\n",
        "    return (dot(u, v) / dot(u, u)) * u # Projection formula is directly applied here\n",
        "\n",
        "u = np.array([1., 0., 0.])\n",
        "v = np.array([2., 3., 4.])\n",
        "p = proj(u, v)\n",
        "check('proj', np.allclose(p, np.array([2., 0., 0.])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4 — Least Squares (Closed Form)\n",
        "\n",
        "### Task 4.1: Solve min_w ||Xw - y||^2\n",
        "\n",
        "Use normal equation: w = (X^T X)^{-1} X^T y\n",
        "\n",
        "# HINT:\n",
        "- Use `np.linalg.solve` (more stable than explicit inverse)\n",
        "\n",
        "**FAANG gotcha:** Don’t compute matrix inverse unless you must."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "w_true [ 1.5 -2.   0.7]\n",
            "w_hat  [ 1.50051512 -2.00176197  0.70098954]\n",
            "L2 error 0.002085441683260867\n",
            "OK: close\n"
          ]
        }
      ],
      "source": [
        "n, d = 50, 3\n",
        "X = rng.standard_normal((n, d))\n",
        "w_true = np.array([1.5, -2.0, 0.7])\n",
        "y = X @ w_true + 0.01 * rng.standard_normal(n)\n",
        "\n",
        "# TODO: compute w_hat using solve\n",
        "w_hat = np.linalg.solve(X.T @ X, X.T @ y) # Using the normal equation to find w_hat as specified\n",
        "\n",
        "err = np.linalg.norm(w_hat - w_true)\n",
        "print('w_true', w_true)\n",
        "print('w_hat ', w_hat)\n",
        "print('L2 error', err)\n",
        "check('close', err < 0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 5 — Eigenvalues & SVD Intuition\n",
        "\n",
        "### Task 5.1: PCA via SVD\n",
        "\n",
        "Steps:\n",
        "1. Center X\n",
        "2. Compute SVD: X = U S V^T\n",
        "3. Take top-k components from V\n",
        "\n",
        "# HINT:\n",
        "- `U, S, Vt = np.linalg.svd(X_centered, full_matrices=False)`\n",
        "\n",
        "**Explain:** Why does SVD show up in embeddings and dimensionality reduction?\n",
        "\n",
        "SVD (Singular Value Decomposition) works on basic idea of spectral decomposition or matrix decomposition(that is, any matrix can be decomposed into a series of linear transformations).\n",
        "\n",
        "SVD identifies the most important patterns and discards the rest. by retaining important features and removing least helpful dimensions we are reducing dimentionality. \n",
        "\n",
        "in case of embeddings, it helps in mapping high dimentaional data (words and their context) to dense & low dimentional vectors (that are embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "relative recon error 0.784408889885354\n",
            "OK: shapes\n"
          ]
        }
      ],
      "source": [
        "X = rng.standard_normal((200, 10))\n",
        "Xc = X - X.mean(axis=0, keepdims=True)  # TODO: center columns\n",
        "U, S, Vt = np.linalg.svd(Xc, full_matrices=False)  # TODO\n",
        "\n",
        "k = 3\n",
        "W = Vt.T[:, :k]  # TODO: top-k right singular vectors (10,k)\n",
        "Z = Xc @ W  # TODO: projection (200,k)\n",
        "X_recon = Z @ W.T  # TODO: reconstruct from top-k\n",
        "\n",
        "recon_err = np.linalg.norm(Xc - X_recon) / np.linalg.norm(Xc)\n",
        "print('relative recon error', recon_err)\n",
        "check('shapes', W.shape == (10, k) and Z.shape == (200, k))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Submission Checklist\n",
        "- All TODOs completed\n",
        "- Checks pass\n",
        "- Explain prompts answered\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
